{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install evaluate\n#!pip install pytorch\nimport pandas as pd\nimport evaluate\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ncsv = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\ncsv['sentiment'] = csv['sentiment'].map({'positive': 1, 'negative': 0})\ncsv = csv.rename(columns={'sentiment': 'label'})\n#csv = csv[:50]\n\ndataset = Dataset.from_dict(csv)\n\n# 70% for training\ntrain_test_split_ratio = 0.7\ntrain_dataset, temp_dataset = dataset.train_test_split(test_size=1-train_test_split_ratio, seed=42).values()\n\n# 15% for validation and testing each\nval_test_split_ratio = 0.5\nvalid_dataset, test_dataset = temp_dataset.train_test_split(test_size=val_test_split_ratio, seed=42).values()\n\n\nmodel_name = \"distilbert/distilbert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef preprocess(examples):\n    return tokenizer(examples[\"review\"], truncation=True, padding=\"max_length\")\n\ntokenized_dataset = dataset.map(preprocess, batched=True)\n\n# 70% for training\ntrain_test_split_ratio = 0.7\ntrain_dataset, temp_dataset = tokenized_dataset.train_test_split(test_size=1-train_test_split_ratio, seed=42).values()\n\n# 15% for validation and testing each\nval_test_split_ratio = 0.5\nvalid_dataset, test_dataset = temp_dataset.train_test_split(test_size=val_test_split_ratio, seed=42).values()\n\n\naccuracy_metric = evaluate.load(\"accuracy\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load(\"recall\")\nf1_metric = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"f1\"]\n\n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    learning_rate=2e-5,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    logging_steps=1,  # Log more frequently\n    report_to=\"none\",  # Avoid sending logs to external services\n    logging_first_step=True  # Log the first step\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\nmodel.save_pretrained(\"/kaggle/working/distilbert\")\ntokenizer.save_pretrained(\"/kaggle/working/distilbert\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T14:23:20.412243Z","iopub.execute_input":"2025-02-02T14:23:20.412537Z","iopub.status.idle":"2025-02-02T14:59:37.254076Z","shell.execute_reply.started":"2025-02-02T14:23:20.412514Z","shell.execute_reply":"2025-02-02T14:59:37.253261Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d01a31a48f348078e3967902865a998"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4376' max='4376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4376/4376 35:46, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.244900</td>\n      <td>0.196159</td>\n      <td>0.929333</td>\n      <td>0.907517</td>\n      <td>0.953324</td>\n      <td>0.929857</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.136400</td>\n      <td>0.208775</td>\n      <td>0.936267</td>\n      <td>0.928400</td>\n      <td>0.943012</td>\n      <td>0.935649</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/distilbert/tokenizer_config.json',\n '/kaggle/working/distilbert/special_tokens_map.json',\n '/kaggle/working/distilbert/vocab.txt',\n '/kaggle/working/distilbert/added_tokens.json',\n '/kaggle/working/distilbert/tokenizer.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:03:51.450499Z","iopub.execute_input":"2025-02-02T15:03:51.450791Z","iopub.status.idle":"2025-02-02T15:03:51.467999Z","shell.execute_reply.started":"2025-02-02T15:03:51.450769Z","shell.execute_reply":"2025-02-02T15:03:51.467074Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc9cb4732e149cd896d55bb75301f33"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"model.push_to_hub(\"sentiment-model\")\ntokenizer.push_to_hub(\"sentiment-model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:30:24.545034Z","iopub.execute_input":"2025-02-02T15:30:24.545400Z","iopub.status.idle":"2025-02-02T15:30:27.260275Z","shell.execute_reply.started":"2025-02-02T15:30:24.545370Z","shell.execute_reply":"2025-02-02T15:30:27.259530Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fbb6d1e126e41d48f70819fee7c1e95"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/wetsq/sentiment-model/commit/98cf479295a9a32f64b686c79977b6fc43779510', commit_message='Upload tokenizer', commit_description='', oid='98cf479295a9a32f64b686c79977b6fc43779510', pr_url=None, repo_url=RepoUrl('https://huggingface.co/wetsq/sentiment-model', endpoint='https://huggingface.co', repo_type='model', repo_id='wetsq/sentiment-model'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"https://huggingface.co/wetsq/sentiment-model\n\nhttps://github.com/wetsq/sentiment-analysis\n\nhttps://www.youtube.com/watch?v=CQhhxNUcHrg","metadata":{}}]}